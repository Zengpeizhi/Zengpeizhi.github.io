---
layout:     post
title:      XGBoost学习笔记
subtitle:   XGBoost大白话讲解
date:       2020-04-10
author:     Zengpeizhi
header-img: img/post-bg-debug.png
catalog: true
tags:
    - 机器学习
    - 数据挖掘
    - XGBoost
    - GBDT
---

# 机器学习：XGBoost学习笔记


### 1.什么是XGBoost

XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。

说到XGBoost，不得不提GBDT(Gradient Boosting Decision Tree)。因为XGBoost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X (Extreme) GBoosted。包括前面说过，两者都是boosting方法。其实除了boosting方法之外，还有一种bagging方法（以随机森林为代表的），这两种优化方法都是如今比较流行的一个思路。

**bagging:**若干个过拟合分类器

**boosting:**若干个欠拟合分类器

#### 1.1XGBoost树的定义以及整个优化的过程

XGBoost的树的定义是基于回归树的，大致的思想是第k棵树的生成是基于前k-1棵树拟合的残差来进行拟合，用到了二阶梯度去拟合残差（GBDT只用到了一阶梯度）。

具体地说，给定一个数据集D中有n个样本，每个样本有m维特征。通过训练数据集D，我们得到K棵树。这K棵树累加的值为我们的预测值，其中$\tilde{y}_{i}$ 代表最终的预测结果，$f_{k}\left(x_{i}\right)$ 代表样本 $x_i$ 在第k棵树上的权重。

$$
\tilde{y}_{i}=\phi\left(x_{i}\right)=\sum_{k=1}^{K} f_{k}\left(x_{i}\right), f_{k} \in \mathcal{F}
$$

假设给定的样本有n个样本，m个特征，则有

$$
\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)
$$

其中 $x_i$ 表示第 i 个样本，$y_i$ 表示第 i 个类别标签，回归树（CART树）的空间为 $\mathcal{F}$ 为：

$$
\mathcal{F}=f(x)=w_{q(x)}\left(q: R^{m} \rightarrow T, w \in R^{T}\right)
$$

简单的理解就是XGBoost中的每一棵树把每个点$x_i$映射到了每棵树的叶子节点所对应的权重。其中$q$代表每棵树的结构，他将样本映射到对应的叶节点； $T$ 是对应树的叶节点个数； $f(x)$ 对应树的结构 $q$ 和叶节点权重 $w$ 。所以XGBoost的预测值是每棵树对应的叶节点的值的和。

而我们的目标是学习这k个树，所以我们最小化下面这个带正则项的目标函数：

$$
L(\phi)=\sum_{i} l\left(\tilde{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)
$$

$$
\text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}
$$

 $\tilde{y}_{i}$ 表示模型的预测值， $y_i$ 代表第 $i$ 个样本的类别标签， $k$ 表示树的数量， $f_k$ 代表第 $k$ 棵树的模型， $T$ 代表每棵树的叶子节点的数量， $w$ 表示每棵树的叶子节点的分数组成的集合， $\gamma $ 和 $\lambda$ 表示惩罚项的参数，这两个数的值在实际使用上是需要调参的。上式的第一项是损失误差，如MSE和logistic损失等，第二项是正则项，控制树的复杂度，防止过拟合。

简单地把上述每一步的优化迭代过程展开如下所示：

![](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb1.jpg?raw=true)



此时损失函数可以写成：

$$
obj=\sum_{i=1}^{n} l\left(y_{i}, \tilde{y}_{i}^{k}\left(x_{i}\right)\right)+\sum_{k=1}^{K}\Omega\left(f_{k}\right)=\sum_{i=1}^{n} l\left(y_{i}, \tilde{y}_{i}^{k-1}+f_{k}\left(x_{i}\right)\right)+\sum_{k=1}^{K-1}\Omega\left(f_{k}\right)
$$

当训练第k棵树的时候，由于前k-1棵树的复杂度损失已经确定了的，可以当做常数，因此第k步的时候优化只需考虑第k棵树的复杂度，因此此时目标函数转化为：

$$
Minimize=\sum_{i=1}^{n} l\left(y_{i}, \tilde{y}_{i}^{k-1}+f_{k}\left(x_{i}\right)\right)+\Omega\left(f_{k}\right)+constant
$$

实质是把样本分配到叶子结点会对应一个obj，优化过程就是obj优化。也就是分裂节点到叶子不同的组合，不同的组合对应不同obj，所有的优化围绕这个思想展开。在第k步的时候，别忘了我们此时想要求解的是第k棵树的预测函数 $f(k)$ 以及 $\Omega(f(k)) $，提取出第t次迭代树的预测值，进行单独优化。这时我们可以利用我们熟悉的二阶泰勒展开进行近似：

$$
Minimize \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \tilde{y}^{k-1}\right)+g_{i} f_{k}\left(x_{i}\right)+\frac{1}{2} h_{i} 
f_{k}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{k}\right)+constant
$$


其中  $g_i$ 为第i个样本预测值在前t次迭代的损失函数的一阶梯度， $h_i$ 为第i个样本预测值在前t次迭代的损失函数的二阶梯度。$g_{i}=\partial_{\tilde{y}^{k-1}} l\left(y_{i}, \tilde{y}^{k-1}\right)$ 第i个样本预测值在前t次迭代的损失函数的一阶梯度。$h_{i}=\partial_{\tilde{y}^{k-1}}^{2} l\left(y_{i}, \tilde{y}^{k-1}\right)$ 为 第i个样本预测值在前t次迭代的损失函数的二阶梯度。**注意，当训练第i棵树的时候， $g_i$ 和 $h_i$ 都是已知的，因为他们是对第i-1棵树求偏导，因此我们关心的是第i棵树的在叶子节点上的预测值以及第i棵树的复杂度即可**

> 不是很懂泰勒展式的可以看这里：
>
> $$
> f(x+\Delta x) \approx f(x)+f^{\prime}(x) \cdot \Delta x+\frac{1}{2} f^{\prime \prime}(x)\Delta x^2
> $$
> 
> 我们令 $f(x) = l\left(y_{i}, \tilde{y_i}^{k-1}\right)$ ， $f(x+\Delta x)= l\left(y_{i}, \tilde{y_i}^{k-1}+f_k(x_i)\right)$ 
>
> 将 $f(x)$ 和 $f(x+\Delta x)$ 带入目标函数即公式（7）进泰勒公式即可得公式（8）。

至此，$f$和$\Omega$的表示方式仍是不清楚的，接下来的任务时求它们。

#### 1.2 树的参数化

整理上一小节所说的我们需要优化的目标函数为：

$$
minimize \sum_{i=1}^{n}\left[g_{i} f_{k}\left(x_{i}\right)+\frac{1}{2} h_{i} 
f_{k}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{k}\right)+constant
$$

注意这里由于表示的是在生成第k步的时候的优化目标，因此已知的部分都可以放到constant常数部分。

这里笔者因为贪方便，参考了一个优秀作者的手写的笔记。

<img src="https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb2.jpg?raw=true" style="zoom: 67%;" />





<img src="https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb3.jpg?raw=true" style="zoom: 67%;" />





<img src="https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb4.jpg?raw=true" style="zoom:67%;" />

很有意思的一个事实，我们从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。对于一个叶子节点如何进行分裂，XGBoost作者在其原始论文中给出了一种分裂节点的方法：**枚举所有不同树结构的贪心法**

不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。这个寻找的过程使用的就是**贪心算法**。选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。

总而言之，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，**遍历所有特征的所有特征划分点**，不同的是使用的目标函数不一样。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。从而继续分裂，形成一棵树，再形成一棵树，**每次在上一次的预测基础上取最优进一步分裂/建树。**

<img src="https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb5.jpg?raw=true" style="zoom:67%;" />



<img src="https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb6.jpg?raw=true" style="zoom:67%;" />

**上图最后显示的就是XGBoost特有的不纯度函数**

#### 1.3 总结一下XGBoost过程

- 构建目标函数
- 利用泰勒技术的形式对目标函数近似
- 将树结构参数化
- 利用贪心算法求解局部最优树

#### 1.4防止过拟合的一些方法

凡是这种循环迭代的方式必定有停止条件，什么时候停止呢？简言之，设置树的最大深度、当样本权重和小于设定阈值时停止生长以防止过拟合。具体而言，则

1. 当引入的分裂带来的增益小于设定阀值的时候，我们可以忽略掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）；
2. 当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合；
3. 样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；

### 2.XGBoost和GBDT的区别

除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。

1. **相比GBDT精度更高**。GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。.XGBoost引入二阶导数一方面是为了增加精度，另一方面是为了能自定义损失函数，二阶泰勒展开可以近似大量损失函数
2. **灵活性更强**。传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。（使用线性分类器的XGBoost相当于带l1和l2正则的逻辑回归或者线性回归）。此外XGBoost还支持自定义损失函数
3. **正则项**：XGBoost在目标函数中加入了正则项，用于控制模型的复杂度。正则项包括树的叶子节点的个数，叶子节点的权重，正则化有利于降低模型的方差，防止过拟合。
4. **shrinkage（缩减因子）**：GBDT也有，即在每一次迭代后，叶子节点权重乘上某个系数，如$F_{(m)}(x)=F_{(m-1)}(x)+v*h_{(m)}(x)$ ,其中$v$即为缩减因子（学习率），一般情况下，较小的学习率可以更好地逼近预测值，不容易过拟合，但迭代次数会增加，常取0.1
5. **支持行采样**：GBDT也有。即从原始数据集中随机抽取一部分子集作为本次需要拟合的样本集。
6. **支持列采样**：GBDT没有。XGBoost则采用了与随机 森林相似的策略，支持对特征进行采样。
7. **缺失值处理**：传统的GBDT没有设计对缺失值进行处理，在代进模型进行计算的时候需要先对数据进行补全缺失值操作，XGBoost能够使用稀疏感知算法自动学习出缺 失值的处理策略。具体地说是先对不含缺失值的样本进行学习分裂，分裂完之后对于有缺失值的样本，则看分到左边的损失小还是右边的损失小。
8. **采用了近似算法选择特征分裂**（在贪心基础上做了改进）提高运算速度。贪心算法是会先对数据进行预排序，遍历每个特征的各个可能的分裂节点，并计算出最优的分裂特征。在数据量大的时候贪心算法无法读入内存计算，而XGBoost的近似算法是只考虑分位点做分裂节点（加权分位数），减少计算量。即先根据特征的分位数提出候选划分点，然后将连续型特征映射到候选点划分的桶中，选出最优分裂点。
9. *8支持并行操作**。这里的并行不是指每棵树的生成是并行的，而是一开始排序结果会按照分位数来保存到各个block中，后面可以根据block的信息去进行分裂

### 3.一些关于XGBoost的问题

#### 3.1 XGBoost的损失函数是什么

虽然XGBoost原理是回归树，但是损失函数的话看具体问题的。对于回归问题，用MSE，等于分类问题，用对数损失或者logistic损失等

#### 3.2 XGBoost为什么要做泰勒展开

XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。

#### 3.3 XGBoost的一些调参技巧及代码实现

参考[这里](https://www.cnblogs.com/TimVerion/p/11436001.html)

#### 一些其他大佬的博客
[手撕XGBoost](https://zhuanlan.zhihu.com/p/119203017)
[调参技巧]: https://www.cnblogs.com/TimVerion/p/11436001.html


