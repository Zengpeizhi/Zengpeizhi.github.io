---
layout:     post
title:      集成学习方法总结
subtitle:   bagging与boosting总结
date:       2020-04-13
author:     Zengpeizhi
header-img: img/post-bg-digital-native.jpg
catalog: true
tags:
    - 集成学习
    - XGBoost
    - RF
    -随机森林
    -bagging
    -boosting
    -GBDT
---


# 集成学习GDBT,XGBOOST,RF

## 前言

集成学习（Ensemble Learning），集成学习的目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性。

集成学习致分为两大类：

- Boosting:即个体学习器之间存在强依赖关系、必须串行生成的序列化方法，Adaboost, GDBT, Xgboost.
- Bagging以及个体学习器间不存在强依赖关系、可同时生成的并行化方法，“随机森林”（Random Forest）。

**Bagging (bootstrap aggregating)**

Bagging即套袋法，其算法过程如下：

A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

**Boosting**

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

1）在每一轮如何改变训练数据的权值或概率分布？

通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

2）通过什么方式来组合弱分类器？

通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

**Bagging，Boosting二者之间的区别**

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

# 1. Bagging

Bagging：简单放回抽样，多数表决（分类）或简单平均（回归）,同时Bagging的基学习器之间属于并列生成，无依赖关系。

## 1.1 随机森林

Random Forest（随机森林）：Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括RF包括四个部分：

1、随机选择 样本（放回抽样）；
2、随机选择 特征；
3、构建分类器；如：ID3、C4.5、CART、SVM、Logistic regression等
4、投票（平均）

> 随机森林的随机性体现在两个方面：
>
> 1. 随机采样：随机森林在计算每棵树时，从全部训练样本（样本数为n）中选取一个可能有重复的、大小同样为n的数据集进行训练（即booststrap采样）。
> 2. 特征选取的随机性：在每个节点随机选取所有特征的一个子集，用来计算最佳分割方式。

#### **随机森林的优点：**

1. 表现性能好，与其他算法相比有着很大优势。
2. 随机森林能处理很高维度的数据（也就是很多特征的数据），并且不用做特征选择。
3. 在训练完之后，随机森林能给出哪些特征比较重要。
4. 训练速度快，容易做成并行化方法(训练时，树与树之间是相互独立的)。
5. 在训练过程中，能够检测到feature之间的影响。
6. 对于不平衡数据集来说，随机森林可以平衡误差。当存在分类不平衡的情况时，随机森林能提供平衡数据集误差的有效方法。
7. 如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。
8. 随机森林算法有很强的抗干扰能力（具体体现在6,7点）。所以当数据存在大量的数据缺失，用RF也是不错的。
9. 随机森林抗过拟合能力比较强（虽然理论上说随机森林不会产生过拟合现象，但是在现实中噪声是不能忽略的，增加树虽然能够减小过拟合，但没有办法完全消除过拟合，无论怎么增加树都不行，再说树的数目也不可能无限增加的。）
10. 随机森林能够解决分类与回归两种类型的问题，并在这两方面都有相当好的估计表现。（虽然RF能做回归问题，但通常都用RF来解决分类问题）。
11. 在创建随机森林时候，对generlization error(泛化误差)使用的是无偏估计模型，泛化能力强。

#### 随机森林的缺点：

1. 随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上会过拟合）。
2. 对于许多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试。
3. 可能有很多相似的决策树，掩盖了真实的结果。
4. 对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。
5. 执行数据虽然比boosting等快（随机森林属于bagging），但比单只决策树慢多了。

> PS:最后几个重要的点
>
> 1.RF采用多个决策树的投票机制来改善决策树。
> 2.为什么不能用全样本去训练m棵决策树？
>         答：全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的（如果有m个决策树，那就需要m个一定数量的样本集来训练每一棵树）
>
> 3.产生n个样本的方法，采用Bootstraping法，这是一种又放回的抽样方法，产生n个样本。
>
> 4.最终采用Bagging的策略来获得，即多数投票机制。

# 2. Boosting

## 2.1 基于调整权重 Adaboost

每生成一棵树之后，计算两个权重

> 1 计算这个树的误差率，误差率越高，权重越低
> 2 计算每个样本的错分率，错分的样本，权重越高，之后更容易分对

![img](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb-rf-1.jpg?raw=true)
![img](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb-rf-2.jpg?raw=true)

更多的关于Adaboost的介绍参考[这里](https://blog.csdn.net/qq_24519677/article/details/81910112)

## 2.2 基于残差：GB(GBTD,Xgboost)

### 2.2.1 GBTD

GBDT只能由回归树组成.

- 基本思想：
  在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法。

![img](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb-rf-3.jpg?raw=true)

- 如果是分类树，损失函数是指数损失函数：
  𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))

- 如果是回归树，损失函数是均方损失（CART）：
  𝐿(𝑦,𝑓(𝑥))=(𝑦−𝑓(𝑥))^2

- 如何防止过拟合？

  > 1. 步长v(0-1)，权重衰减 𝑓𝑘(𝑥)=𝑓𝑘−1(𝑥)+𝑣 ℎ𝑘(𝑥)，降低新来的分类器的影响力
  > 2. 子采样比例
  > 3. 用弱学习器

### 2.2.2 xgboost

当已经生成了一棵树的时候，如何去选择新子树：

- 1 目标函数：

  Obj=∑i=1nl(yi,y^i)+∑kΩ(fk),fk∈FObj=∑i=1nl(yi,y^i)+∑kΩ(fk),fk∈F

- 1.1 第t轮的时候：

  Obj(t)=∑i=1nl(yi,y^(t)i)+∑i=1tΩ(fi)≡∑i=1nl(yi,y^(t−1)i+ft(xi))+Ω(ft)+cObj(t)=∑i=1nl(yi,y^i(t))+∑i=1tΩ(fi)≡∑i=1nl(yi,y^i(t−1)+ft(xi))+Ω(ft)+c

- 这时候需要寻找f_t来让目标函数最小

> a. 式子左边，对目标函数在ft(x)ft(x)上泰勒展开，去二阶，求得近似解：
> ![img](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb-rf-4.jpg?raw=true)
>
> b. 式子右边，定义复杂度 Ω(ft)Ω(ft)
> 每颗树，都是由枝干(分类节点)和叶子(树的末端)组成的。
> 定义复杂度为：叶子个个数T, 加上每个叶子的值w平方和（各有系数）。
> 树越复杂，T ↑，w平方和 ↑，复杂度 ↑，惩罚 ↑。
> ![img](https://liyuanimage.oss-cn-beijing.aliyuncs.com/img/20191116121238.png)

![image-20200413141841048](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb-rf-5.jpg?raw=true)

------

如何生成候选树？

- Enumerate 枚举可能的结构
- 通过刚才的式子计算最优分数
- 但是问题是有无限的可能性。

------

- 所以通过贪婪学习：（不详细讲）
  每一次尝试对已有的叶子结点加入一个分割，选择具有最佳增益的分割对结点进行分裂。对于一个具体的分割方案，我们可以获得的增益可以由如下公式计算：
  ![img](https://github.com/Zengpeizhi/Zengpeizhi.github.io/blob/master/img/xgb-rf-6.jpg?raw=true)

> 也就是通过信息增益去寻找最优分割点。
> 这里有个好处就是如果惩罚大于增益，gain就会为负数，自动停止。

# 3 模型对比

## 3.1 随机森林vsGDBT

- 决策树类型：组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成；
- 结果预测：对于最终的输出结果而言，随机森林采用多数投票、简单平均等；而GBDT则是将所有结果累加起来，或者加权累加起来；
- 并行/串行：组成随机森林的树可以并行生成；而GBDT只能是串行生成；
- 异常值：随机森林对异常值不敏感；GBDT对异常值非常敏感；
- 方差/偏差：随机森林减少方差；GBDT是通过减少偏差。

## 3.2 GDBT vs XGboost

- GDBT只支持CATR树，xgboost还支持线性分类器
- GDBT只用了一阶，xgboost泰勒展开，用了二阶
- xgboost有正则项，而且会自动停止生成(依赖参数gamma)。
- xgboost可以列抽样，借鉴了随机森林的做法
- XGBOOST可以自动学习出缺失值的分裂方向
- XGBOOST实现了并行化：每个特征并行计算，每个特征划分也并行计算

后期实现中，xgboost还有优化，所以很快：

- 在寻找分割点，枚举贪心法效率低，xgboost实现近似的算法。大致的思想是根据百分位法列举几个成为分割点的候选者，然后再进一步计算。
- xgboost考虑了训练数据为稀疏值的情况
- 特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。
- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
